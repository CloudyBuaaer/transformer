{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CloudyBuaaer/transformer/blob/main/%E4%BD%BF%E7%94%A8_LLaMA_Factory_%E5%BE%AE%E8%B0%83_Llama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用 LLaMA Factory 微调 Llama-3 中文对话模型\n",
        "\n",
        "请申请一个免费 T4 GPU 来运行该脚本\n",
        "\n",
        "项目主页: https://github.com/hiyouga/LLaMA-Factory\n"
      ],
      "metadata": {
        "id": "gf60HoT633NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安装 LLaMA Factory 依赖"
      ],
      "metadata": {
        "id": "rMp4rTWk4TKZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yQDp0sXX3qE4",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5dc9f60-33bf-4bc7-c348-1ef78b8c0eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 629, done.\u001b[K\n",
            "remote: Counting objects: 100% (629/629), done.\u001b[K\n",
            "remote: Compressing objects: 100% (468/468), done.\u001b[K\n",
            "remote: Total 629 (delta 154), reused 400 (delta 103), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (629/629), 5.25 MiB | 18.29 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/    LICENSE      pyproject.toml  \u001b[01;34mrequirements\u001b[0m/  \u001b[01;34mtests\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mdocs\u001b[0m/      Makefile     README.md       \u001b[01;34mscripts\u001b[0m/       \u001b[01;34mtests_v1\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/  MANIFEST.in  README_zh.md    \u001b[01;34msrc\u001b[0m/\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate<=1.11.0,>=1.3.0 (from llamafactory==0.9.5.dev0)\n",
            "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting av<=16.0.0,>=10.0.0 (from llamafactory==0.9.5.dev0)\n",
            "  Downloading av-16.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: datasets<=4.0.0,>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (4.0.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.8.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.128.2)\n",
            "Collecting fire (from llamafactory==0.9.5.dev0)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: gradio<=5.50.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (5.50.0)\n",
            "Collecting hf-transfer (from llamafactory==0.9.5.dev0)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (3.10.0)\n",
            "Collecting modelscope (from llamafactory==0.9.5.dev0)\n",
            "  Downloading modelscope-1.34.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.0.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (26.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.2.2)\n",
            "Requirement already satisfied: peft<=0.18.1,>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.18.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (5.29.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (6.0.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.2.1)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (3.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.12.0)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchaudio>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchdata<=0.11.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.11.0)\n",
            "Requirement already satisfied: torchvision>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.24.0+cu128)\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (5.0.0)\n",
            "Collecting trl<=0.24.0,>=0.18.0 (from llamafactory==0.9.5.dev0)\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.5.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.40.0)\n",
            "\u001b[33mWARNING: llamafactory 0.9.5.dev0 does not provide the extra 'bitsandbytes'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: llamafactory 0.9.5.dev0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (1.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (2025.3.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.2.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.11.7)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.0.22)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.15.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (4.15.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (15.0.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->llamafactory==0.9.5.dev0) (0.4.2)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->llamafactory==0.9.5.dev0) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->llamafactory==0.9.5.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->llamafactory==0.9.5.dev0) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->llamafactory==0.9.5.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->llamafactory==0.9.5.dev0) (2.41.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (3.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (3.5.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata<=0.11.0,>=0.10.0->llamafactory==0.9.5.dev0) (2.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0->llamafactory==0.9.5.dev0) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0->llamafactory==0.9.5.dev0) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0->llamafactory==0.9.5.dev0) (0.21.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.5.dev0) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.5.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.5.dev0)\n",
            "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.5.dev0) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.5.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->llamafactory==0.9.5.dev0) (3.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->llamafactory==0.9.5.dev0) (4.9.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.11)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (1.5.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.5.dev0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.5.dev0) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->llamafactory==0.9.5.dev0) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (1.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.5.dev0) (0.1.2)\n",
            "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-16.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.34.0-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.5.dev0-py3-none-any.whl size=26985 sha256=c0f4f5ccfa86710b063aed821d9a303dac59ec1d3efe866c72e957db0e39a8d0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1662yf4g/wheels/68/8b/5e/52f9888e6a91a2651260d603137c052b925af896da6e32a3f7\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: shtab, hf-transfer, fire, av, modelscope, tyro, accelerate, trl, llamafactory\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "Successfully installed accelerate-1.11.0 av-16.0.0 fire-0.7.1 hf-transfer-0.1.9 llamafactory-0.9.5.dev0 modelscope-1.34.0 shtab-1.8.0 trl-0.24.0 tyro-0.8.14\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 检查 GPU 环境\n",
        "\n",
        "免费 T4 申请教程：https://zhuanlan.zhihu.com/p/642542618"
      ],
      "metadata": {
        "id": "Gs68aSwm5MFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"需要 GPU 环境，申请教程：https://zhuanlan.zhihu.com/p/642542618\")"
      ],
      "metadata": {
        "id": "P6rwbyFa5LkF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 更新自我认知数据集\n",
        "\n",
        "可以自由修改 NAME 和 AUTHOR 变量的内容。"
      ],
      "metadata": {
        "id": "gYEF3SGI6XdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-Chinese\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "7gX4PskL6UJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用 LLaMA Board Web UI 微调模型"
      ],
      "metadata": {
        "id": "cA7vWZ6om3cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YIfzFgLsm2kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用命令行微调模型\n",
        "\n",
        "微调过程大约需要 30 分钟。"
      ],
      "metadata": {
        "id": "B6ap81295trx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # 进行指令监督微调\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
        "  dataset=\"identity,alpaca_en_demo,alpaca_zh_demo\",          # 使用 alpaca 和自我认知数据集\n",
        "  template=\"llama3\",                                         # 使用 llama3 提示词模板\n",
        "  finetuning_type=\"lora\",                                    # 使用 LoRA 适配器来节省显存\n",
        "  lora_target=\"all\",                                         # 添加 LoRA 适配器至全部线性层\n",
        "  output_dir=\"llama3_lora\",                                  # 保存 LoRA 适配器的路径\n",
        "  per_device_train_batch_size=2,                             # 批处理大小\n",
        "  gradient_accumulation_steps=4,                             # 梯度累积步数\n",
        "  lr_scheduler_type=\"cosine\",                                # 使用余弦学习率退火算法\n",
        "  logging_steps=5,                                           # 每 5 步输出一个记录\n",
        "  warmup_ratio=0.1,                                          # 使用预热学习率\n",
        "  save_steps=1000,                                           # 每 1000 步保存一个检查点\n",
        "  learning_rate=5e-5,                                        # 学习率大小\n",
        "  num_train_epochs=3.0,                                      # 训练轮数\n",
        "  max_samples=300,                                           # 使用每个数据集中的 300 条样本\n",
        "  max_grad_norm=1.0,                                         # 将梯度范数裁剪至 1.0\n",
        "  loraplus_lr_ratio=16.0,                                    # 使用 LoRA+ 算法并设置 lambda=16.0\n",
        "  fp16=True,                                                 # 使用 float16 混合精度训练\n",
        "  report_to=\"none\",                                          # 关闭 wandb 记录器\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "psywJyo75vt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型推理"
      ],
      "metadata": {
        "id": "otpDQuzaMBpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
        "  adapter_name_or_path=\"llama3_lora\",                        # 加载之前保存的 LoRA 适配器\n",
        "  template=\"llama3\",                                         # 和训练保持一致\n",
        "  finetuning_type=\"lora\",                                    # 和训练保持一致\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"使用 `clear` 清除对话历史，使用 `exit` 退出程序。\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"对话历史已清除\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "id": "kbFsAE-y5so4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并 LoRA 权重并上传模型\n",
        "\n",
        "注意：Colab 免费版仅提供了 12GB 系统内存，而合并 8B 模型的 LoRA 权重需要至少 18GB 系统内存，因此你 **无法** 在免费版运行此功能。"
      ],
      "metadata": {
        "id": "_7g7kprbhXo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "XA2kyAz-hXbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # 使用非量化的官方 Llama-3-8B-Instruct 模型\n",
        "  adapter_name_or_path=\"llama3_lora\",                       # 加载之前保存的 LoRA 适配器\n",
        "  template=\"llama3\",                                        # 和训练保持一致\n",
        "  finetuning_type=\"lora\",                                   # 和训练保持一致\n",
        "  export_dir=\"llama3_lora_merged\",                          # 合并后模型的保存目录\n",
        "  export_size=2,                                            # 合并后模型每个权重文件的大小（单位：GB）\n",
        "  export_device=\"cpu\",                                      # 合并模型使用的设备：`cpu` 或 `auto`\n",
        "  # export_hub_model_id=\"your_id/your_model\",               # 用于上传模型的 HuggingFace 模型 ID\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "eERYoAOrhpcu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}