{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6aca3da5b3e04911ad8f286e64789411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba3a23b7523547eca4d38f312bc79701",
              "IPY_MODEL_f6eb79b59dfb494b9b6c654fe5893a6b",
              "IPY_MODEL_f5ecaa73c1a04fe2acda2b08897e4b71"
            ],
            "layout": "IPY_MODEL_e79514e8e2d142c4b19eec0143e4ba7b"
          }
        },
        "ba3a23b7523547eca4d38f312bc79701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ea301eb547045ae8ff666aaaccf09f2",
            "placeholder": "​",
            "style": "IPY_MODEL_46cec6cba43a40edb7e496bc923f06ea",
            "value": "Loading weights: 100%"
          }
        },
        "f6eb79b59dfb494b9b6c654fe5893a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b84b0941e8154eed82ec68375708f16a",
            "max": 291,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d3ea58b22b542bb94fc349ca50c8f3d",
            "value": 291
          }
        },
        "f5ecaa73c1a04fe2acda2b08897e4b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_610f3d3848d849e690f603af49869270",
            "placeholder": "​",
            "style": "IPY_MODEL_3aa8b821345d42bb959ded291dce9560",
            "value": " 291/291 [00:11&lt;00:00, 19.46it/s, Materializing param=model.norm.weight]"
          }
        },
        "e79514e8e2d142c4b19eec0143e4ba7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea301eb547045ae8ff666aaaccf09f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46cec6cba43a40edb7e496bc923f06ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b84b0941e8154eed82ec68375708f16a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d3ea58b22b542bb94fc349ca50c8f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "610f3d3848d849e690f603af49869270": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa8b821345d42bb959ded291dce9560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CloudyBuaaer/transformer/blob/main/%E4%BD%BF%E7%94%A8LLaMA_Factory%E5%BE%AE%E8%B0%83Llama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用 LLaMA Factory 微调 Llama-3 中文对话模型\n",
        "\n",
        "请申请一个免费 T4 GPU 来运行该脚本\n",
        "\n",
        "项目主页: https://github.com/hiyouga/LLaMA-Factory\n"
      ],
      "metadata": {
        "id": "gf60HoT633NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安装 LLaMA Factory 依赖"
      ],
      "metadata": {
        "id": "rMp4rTWk4TKZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yQDp0sXX3qE4",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5dc9f60-33bf-4bc7-c348-1ef78b8c0eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 629, done.\u001b[K\n",
            "remote: Counting objects: 100% (629/629), done.\u001b[K\n",
            "remote: Compressing objects: 100% (468/468), done.\u001b[K\n",
            "remote: Total 629 (delta 154), reused 400 (delta 103), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (629/629), 5.25 MiB | 18.29 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/    LICENSE      pyproject.toml  \u001b[01;34mrequirements\u001b[0m/  \u001b[01;34mtests\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mdocs\u001b[0m/      Makefile     README.md       \u001b[01;34mscripts\u001b[0m/       \u001b[01;34mtests_v1\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/  MANIFEST.in  README_zh.md    \u001b[01;34msrc\u001b[0m/\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate<=1.11.0,>=1.3.0 (from llamafactory==0.9.5.dev0)\n",
            "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting av<=16.0.0,>=10.0.0 (from llamafactory==0.9.5.dev0)\n",
            "  Downloading av-16.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: datasets<=4.0.0,>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (4.0.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.8.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.128.2)\n",
            "Collecting fire (from llamafactory==0.9.5.dev0)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: gradio<=5.50.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (5.50.0)\n",
            "Collecting hf-transfer (from llamafactory==0.9.5.dev0)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (3.10.0)\n",
            "Collecting modelscope (from llamafactory==0.9.5.dev0)\n",
            "  Downloading modelscope-1.34.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.0.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (26.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.2.2)\n",
            "Requirement already satisfied: peft<=0.18.1,>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.18.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (5.29.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (6.0.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.2.1)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (3.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.12.0)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchaudio>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchdata<=0.11.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.11.0)\n",
            "Requirement already satisfied: torchvision>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.24.0+cu128)\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (5.0.0)\n",
            "Collecting trl<=0.24.0,>=0.18.0 (from llamafactory==0.9.5.dev0)\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.5.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.40.0)\n",
            "\u001b[33mWARNING: llamafactory 0.9.5.dev0 does not provide the extra 'bitsandbytes'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: llamafactory 0.9.5.dev0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (1.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (2025.3.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.2.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.11.7)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.0.22)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.15.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (4.15.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (15.0.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->llamafactory==0.9.5.dev0) (0.4.2)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->llamafactory==0.9.5.dev0) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->llamafactory==0.9.5.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->llamafactory==0.9.5.dev0) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->llamafactory==0.9.5.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->llamafactory==0.9.5.dev0) (2.41.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (3.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (3.5.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata<=0.11.0,>=0.10.0->llamafactory==0.9.5.dev0) (2.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0->llamafactory==0.9.5.dev0) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0->llamafactory==0.9.5.dev0) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0->llamafactory==0.9.5.dev0) (0.21.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.5.dev0) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.5.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.5.dev0)\n",
            "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.5.dev0) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.5.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->llamafactory==0.9.5.dev0) (3.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->llamafactory==0.9.5.dev0) (4.9.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.11)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (1.5.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.5.dev0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.5.dev0) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->llamafactory==0.9.5.dev0) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (1.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.5.dev0) (0.1.2)\n",
            "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-16.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.34.0-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.5.dev0-py3-none-any.whl size=26985 sha256=c0f4f5ccfa86710b063aed821d9a303dac59ec1d3efe866c72e957db0e39a8d0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1662yf4g/wheels/68/8b/5e/52f9888e6a91a2651260d603137c052b925af896da6e32a3f7\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: shtab, hf-transfer, fire, av, modelscope, tyro, accelerate, trl, llamafactory\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "Successfully installed accelerate-1.11.0 av-16.0.0 fire-0.7.1 hf-transfer-0.1.9 llamafactory-0.9.5.dev0 modelscope-1.34.0 shtab-1.8.0 trl-0.24.0 tyro-0.8.14\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes>=0.46.1"
      ],
      "metadata": {
        "id": "u-eTs-y6mbCD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 检查 GPU 环境\n",
        "\n",
        "免费 T4 申请教程：https://zhuanlan.zhihu.com/p/642542618"
      ],
      "metadata": {
        "id": "Gs68aSwm5MFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"需要 GPU 环境，申请教程：https://zhuanlan.zhihu.com/p/642542618\")"
      ],
      "metadata": {
        "id": "P6rwbyFa5LkF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 更新自我认知数据集\n",
        "\n",
        "可以自由修改 NAME 和 AUTHOR 变量的内容。"
      ],
      "metadata": {
        "id": "gYEF3SGI6XdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-Chinese\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "7gX4PskL6UJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1faff05-51c0-41dd-bce5-d8dbf58af5c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用 LLaMA Board Web UI 微调模型"
      ],
      "metadata": {
        "id": "cA7vWZ6om3cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YIfzFgLsm2kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用命令行微调模型\n",
        "\n",
        "微调过程大约需要 30 分钟。"
      ],
      "metadata": {
        "id": "B6ap81295trx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # 进行指令监督微调\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
        "  dataset=\"identity,alpaca_en_demo,alpaca_zh_demo\",          # 使用 alpaca 和自我认知数据集\n",
        "  template=\"llama3\",                                         # 使用 llama3 提示词模板\n",
        "  finetuning_type=\"lora\",                                    # 使用 LoRA 适配器来节省显存\n",
        "  lora_target=\"all\",                                         # 添加 LoRA 适配器至全部线性层\n",
        "  output_dir=\"llama3_lora\",                                  # 保存 LoRA 适配器的路径\n",
        "  per_device_train_batch_size=2,                             # 批处理大小\n",
        "  gradient_accumulation_steps=4,                             # 梯度累积步数\n",
        "  lr_scheduler_type=\"cosine\",                                # 使用余弦学习率退火算法\n",
        "  logging_steps=5,                                           # 每 5 步输出一个记录\n",
        "  warmup_ratio=0.1,                                          # 使用预热学习率\n",
        "  save_steps=1000,                                           # 每 1000 步保存一个检查点\n",
        "  learning_rate=5e-5,                                        # 学习率大小\n",
        "  num_train_epochs=3.0,                                      # 训练轮数\n",
        "  max_samples=300,                                           # 使用每个数据集中的 300 条样本\n",
        "  max_grad_norm=1.0,                                         # 将梯度范数裁剪至 1.0\n",
        "  loraplus_lr_ratio=16.0,                                    # 使用 LoRA+ 算法并设置 lambda=16.0\n",
        "  fp16=True,                                                 # 使用 float16 混合精度训练\n",
        "  report_to=\"none\",                                          # 关闭 wandb 记录器\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "psywJyo75vt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train \\\n",
        "    --model_name_or_path unsloth/llama-3-8b-Instruct-bnb-4bit \\\n",
        "    --stage sft \\\n",
        "    --do_train \\\n",
        "    --dataset identity,alpaca_en_demo,alpaca_zh_demo \\\n",
        "    --template llama3 \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target all \\\n",
        "    --output_dir llama3_lora \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 5 \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 300 \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --loraplus_lr_ratio 16.0 \\\n",
        "    --fp16 \\\n",
        "    --report_to none"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x2Fn7komMxF",
        "outputId": "1caf552b-8821-4dfe-a8d5-0feb44485c30"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
            "[INFO|2026-02-14 10:04:17] llamafactory.hparams.parser:459 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 10:04:18,052 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:04:18,054 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 10:04:21,892 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:04:21,893 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 10:04:22,126 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:04:22,126 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2026-02-14 10:04:24] llamafactory.data.template:144 >> Add <|eom_id|> to stop words.\n",
            "[WARNING|2026-02-14 10:04:24] llamafactory.data.template:149 >> New tokens have been added, make sure `resize_vocab` is True.\n",
            "[INFO|2026-02-14 10:04:24] llamafactory.data.loader:144 >> Loading dataset identity.json...\n",
            "[INFO|2026-02-14 10:04:25] llamafactory.data.loader:144 >> Loading dataset alpaca_en_demo.json...\n",
            "[INFO|2026-02-14 10:04:25] llamafactory.data.loader:144 >> Loading dataset alpaca_zh_demo.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 30653, 7496, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am Llama-Chinese, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 30653, 7496, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am Llama-Chinese, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 10:04:26,384 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:04:26,385 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2026-02-14 10:04:26] llamafactory.model.model_utils.quantization:144 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[INFO|2026-02-14 10:04:26] llamafactory.model.model_utils.kv_cache:144 >> KV cache is disabled during training.\n",
            "[INFO|quantization_config.py:486] 2026-02-14 10:04:27,194 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|auto.py:249] 2026-02-14 10:04:27,194 >> \n",
            "model.safetensors: 100% 5.70G/5.70G [00:53<00:00, 108MB/s]\n",
            "[INFO|modeling_utils.py:732] 2026-02-14 10:05:25,545 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
            "[INFO|modeling_utils.py:801] 2026-02-14 10:05:25,545 >> Will use dtype=torch.bfloat16 as defined in model's config object\n",
            "[INFO|configuration_utils.py:1014] 2026-02-14 10:05:25,546 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading weights: 100% 291/291 [00:22<00:00, 12.99it/s, Materializing param=model.norm.weight]\n",
            "generation_config.json: 100% 220/220 [00:00<00:00, 1.29MB/s]\n",
            "[INFO|configuration_utils.py:967] 2026-02-14 10:05:48,832 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
            "[INFO|configuration_utils.py:1014] 2026-02-14 10:05:48,833 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2026-02-14 10:05:48] llamafactory.model.model_utils.checkpointing:144 >> Gradient checkpointing enabled.\n",
            "[INFO|2026-02-14 10:05:48] llamafactory.model.model_utils.attention:144 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2026-02-14 10:05:48] llamafactory.model.adapter:144 >> Upcasting trainable params to float32.\n",
            "[INFO|2026-02-14 10:05:48] llamafactory.model.adapter:144 >> Fine-tuning method: LoRA\n",
            "[INFO|2026-02-14 10:05:48] llamafactory.model.model_utils.misc:144 >> Found linear modules: o_proj,up_proj,v_proj,gate_proj,k_proj,down_proj,q_proj\n",
            "[INFO|2026-02-14 10:05:49] llamafactory.model.loader:144 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "[INFO|2026-02-14 10:05:50] llamafactory.train.trainer_utils:144 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2383] 2026-02-14 10:05:50,046 >> ***** Running training *****\n",
            "[INFO|trainer.py:2384] 2026-02-14 10:05:50,046 >>   Num examples = 691\n",
            "[INFO|trainer.py:2385] 2026-02-14 10:05:50,046 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2386] 2026-02-14 10:05:50,046 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2389] 2026-02-14 10:05:50,046 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2390] 2026-02-14 10:05:50,046 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2391] 2026-02-14 10:05:50,047 >>   Total optimization steps = 261\n",
            "[INFO|trainer.py:2392] 2026-02-14 10:05:50,051 >>   Number of trainable parameters = 20,971,520\n",
            "{'loss': '1.53', 'grad_norm': '0.7514', 'learning_rate': '7.407e-06', 'epoch': '0.0578'}\n",
            "{'loss': '1.362', 'grad_norm': '1.523', 'learning_rate': '1.667e-05', 'epoch': '0.1156'}\n",
            "{'loss': '1.33', 'grad_norm': '1.01', 'learning_rate': '2.593e-05', 'epoch': '0.1734'}\n",
            "{'loss': '1.247', 'grad_norm': '1.061', 'learning_rate': '3.519e-05', 'epoch': '0.2312'}\n",
            "{'loss': '1.305', 'grad_norm': '0.5063', 'learning_rate': '4.444e-05', 'epoch': '0.289'}\n",
            "{'loss': '1.182', 'grad_norm': '0.7232', 'learning_rate': '4.999e-05', 'epoch': '0.3468'}\n",
            "{'loss': '1.234', 'grad_norm': '1.3', 'learning_rate': '4.989e-05', 'epoch': '0.4046'}\n",
            "{'loss': '1.283', 'grad_norm': '0.9672', 'learning_rate': '4.968e-05', 'epoch': '0.4624'}\n",
            "{'loss': '1.214', 'grad_norm': '0.6463', 'learning_rate': '4.935e-05', 'epoch': '0.5202'}\n",
            "{'loss': '1.11', 'grad_norm': '0.579', 'learning_rate': '4.892e-05', 'epoch': '0.578'}\n",
            "{'loss': '1.21', 'grad_norm': '0.734', 'learning_rate': '4.838e-05', 'epoch': '0.6358'}\n",
            "{'loss': '1.223', 'grad_norm': '1.234', 'learning_rate': '4.773e-05', 'epoch': '0.6936'}\n",
            "{'loss': '1.141', 'grad_norm': '0.4473', 'learning_rate': '4.698e-05', 'epoch': '0.7514'}\n",
            "{'loss': '1.194', 'grad_norm': '0.7883', 'learning_rate': '4.613e-05', 'epoch': '0.8092'}\n",
            "{'loss': '1.194', 'grad_norm': '1.594', 'learning_rate': '4.519e-05', 'epoch': '0.8671'}\n",
            "{'loss': '1.335', 'grad_norm': '1.083', 'learning_rate': '4.415e-05', 'epoch': '0.9249'}\n",
            "{'loss': '1.3', 'grad_norm': '0.7248', 'learning_rate': '4.303e-05', 'epoch': '0.9827'}\n",
            "{'loss': '1.012', 'grad_norm': '1.45', 'learning_rate': '4.183e-05', 'epoch': '1.035'}\n",
            "{'loss': '0.877', 'grad_norm': '1.569', 'learning_rate': '4.055e-05', 'epoch': '1.092'}\n",
            "{'loss': '0.9077', 'grad_norm': '0.6177', 'learning_rate': '3.92e-05', 'epoch': '1.15'}\n",
            "{'loss': '0.9415', 'grad_norm': '1.462', 'learning_rate': '3.779e-05', 'epoch': '1.208'}\n",
            "{'loss': '0.7398', 'grad_norm': '0.5011', 'learning_rate': '3.632e-05', 'epoch': '1.266'}\n",
            "{'loss': '0.9193', 'grad_norm': '1.436', 'learning_rate': '3.48e-05', 'epoch': '1.324'}\n",
            "{'loss': '0.7528', 'grad_norm': '1.432', 'learning_rate': '3.323e-05', 'epoch': '1.382'}\n",
            "{'loss': '0.7903', 'grad_norm': '0.8875', 'learning_rate': '3.163e-05', 'epoch': '1.439'}\n",
            "{'loss': '0.7851', 'grad_norm': '0.4888', 'learning_rate': '3e-05', 'epoch': '1.497'}\n",
            "{'loss': '0.7637', 'grad_norm': '0.5618', 'learning_rate': '2.835e-05', 'epoch': '1.555'}\n",
            "{'loss': '0.6647', 'grad_norm': '0.587', 'learning_rate': '2.668e-05', 'epoch': '1.613'}\n",
            "{'loss': '0.8107', 'grad_norm': '0.9619', 'learning_rate': '2.5e-05', 'epoch': '1.671'}\n",
            "{'loss': '0.7101', 'grad_norm': '0.7591', 'learning_rate': '2.332e-05', 'epoch': '1.728'}\n",
            "{'loss': '0.8337', 'grad_norm': '0.8639', 'learning_rate': '2.165e-05', 'epoch': '1.786'}\n",
            "{'loss': '0.7704', 'grad_norm': '1.164', 'learning_rate': '2e-05', 'epoch': '1.844'}\n",
            "{'loss': '0.7678', 'grad_norm': '1.048', 'learning_rate': '1.837e-05', 'epoch': '1.902'}\n",
            "{'loss': '0.7098', 'grad_norm': '1.182', 'learning_rate': '1.677e-05', 'epoch': '1.96'}\n",
            "{'loss': '0.7472', 'grad_norm': '0.6591', 'learning_rate': '1.52e-05', 'epoch': '2.012'}\n",
            "{'loss': '0.5003', 'grad_norm': '0.8953', 'learning_rate': '1.368e-05', 'epoch': '2.069'}\n",
            "{'loss': '0.4539', 'grad_norm': '0.787', 'learning_rate': '1.221e-05', 'epoch': '2.127'}\n",
            "{'loss': '0.4292', 'grad_norm': '1.134', 'learning_rate': '1.08e-05', 'epoch': '2.185'}\n",
            "{'loss': '0.4799', 'grad_norm': '1.162', 'learning_rate': '9.45e-06', 'epoch': '2.243'}\n",
            "{'loss': '0.4699', 'grad_norm': '0.7962', 'learning_rate': '8.172e-06', 'epoch': '2.301'}\n",
            "{'loss': '0.5634', 'grad_norm': '0.6672', 'learning_rate': '6.97e-06', 'epoch': '2.358'}\n",
            "{'loss': '0.4626', 'grad_norm': '0.9584', 'learning_rate': '5.849e-06', 'epoch': '2.416'}\n",
            "{'loss': '0.4535', 'grad_norm': '1.066', 'learning_rate': '4.814e-06', 'epoch': '2.474'}\n",
            "{'loss': '0.3825', 'grad_norm': '1.045', 'learning_rate': '3.87e-06', 'epoch': '2.532'}\n",
            "{'loss': '0.4157', 'grad_norm': '0.8856', 'learning_rate': '3.022e-06', 'epoch': '2.59'}\n",
            "{'loss': '0.354', 'grad_norm': '0.731', 'learning_rate': '2.272e-06', 'epoch': '2.647'}\n",
            "{'loss': '0.4298', 'grad_norm': '0.8943', 'learning_rate': '1.625e-06', 'epoch': '2.705'}\n",
            "{'loss': '0.4223', 'grad_norm': '0.8179', 'learning_rate': '1.083e-06', 'epoch': '2.763'}\n",
            "{'loss': '0.415', 'grad_norm': '0.6526', 'learning_rate': '6.483e-07', 'epoch': '2.821'}\n",
            "{'loss': '0.5104', 'grad_norm': '0.8718', 'learning_rate': '3.237e-07', 'epoch': '2.879'}\n",
            "{'loss': '0.4113', 'grad_norm': '0.9951', 'learning_rate': '1.103e-07', 'epoch': '2.936'}\n",
            "{'loss': '0.4139', 'grad_norm': '0.9328', 'learning_rate': '9.012e-09', 'epoch': '2.994'}\n",
            "100% 261/261 [42:35<00:00,  6.68s/it][INFO|trainer.py:4115] 2026-02-14 10:48:25,737 >> Saving model checkpoint to llama3_lora/checkpoint-261\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 10:48:26,266 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:48:26,267 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:3327] 2026-02-14 10:48:26,445 >> chat template saved in llama3_lora/checkpoint-261/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2181] 2026-02-14 10:48:26,446 >> tokenizer config file saved in llama3_lora/checkpoint-261/tokenizer_config.json\n",
            "[INFO|trainer.py:2657] 2026-02-14 10:48:29,456 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': '2559', 'train_samples_per_second': '0.81', 'train_steps_per_second': '0.102', 'train_loss': '0.8347', 'epoch': '3'}\n",
            "100% 261/261 [42:39<00:00,  9.81s/it]\n",
            "[INFO|trainer.py:4115] 2026-02-14 10:48:29,458 >> Saving model checkpoint to llama3_lora\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 10:48:29,994 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:48:29,995 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:3327] 2026-02-14 10:48:31,042 >> chat template saved in llama3_lora/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2181] 2026-02-14 10:48:31,043 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 18883355GF\n",
            "  train_loss               =     0.8347\n",
            "  train_runtime            = 0:42:39.40\n",
            "  train_samples_per_second =       0.81\n",
            "  train_steps_per_second   =      0.102\n",
            "[INFO|modelcard.py:266] 2026-02-14 10:48:31,929 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型推理"
      ],
      "metadata": {
        "id": "otpDQuzaMBpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/LLaMA-Factory/src')\n",
        "\n",
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
        "  adapter_name_or_path=\"llama3_lora\",                        # 加载之前保存的 LoRA 适配器\n",
        "  template=\"llama3\",                                         # 和训练保持一致\n",
        "  finetuning_type=\"lora\",                                    # 和训练保持一致\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"使用 `clear` 清除对话历史，使用 `exit` 退出程序。\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"对话历史已清除\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "id": "kbFsAE-y5so4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6aca3da5b3e04911ad8f286e64789411",
            "ba3a23b7523547eca4d38f312bc79701",
            "f6eb79b59dfb494b9b6c654fe5893a6b",
            "f5ecaa73c1a04fe2acda2b08897e4b71",
            "e79514e8e2d142c4b19eec0143e4ba7b",
            "3ea301eb547045ae8ff666aaaccf09f2",
            "46cec6cba43a40edb7e496bc923f06ea",
            "b84b0941e8154eed82ec68375708f16a",
            "2d3ea58b22b542bb94fc349ca50c8f3d",
            "610f3d3848d849e690f603af49869270",
            "3aa8b821345d42bb959ded291dce9560"
          ]
        },
        "collapsed": true,
        "outputId": "b1635725-5b49-486c-f9b5-248c2ee29691"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 10:59:29,603 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:59:29,608 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 10:59:33,511 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:59:33,513 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 10:59:33,755 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:59:33,757 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO|2026-02-14 10:59:36] llamafactory.data.template:144 >> Add <|eom_id|> to stop words.\n",
            "[WARNING|2026-02-14 10:59:36] llamafactory.data.template:149 >> New tokens have been added, make sure `resize_vocab` is True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:667] 2026-02-14 10:59:36,451 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 10:59:36,453 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:327] 2026-02-14 10:59:36,454 >> `torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO|2026-02-14 10:59:36] llamafactory.model.model_utils.quantization:144 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[INFO|2026-02-14 10:59:36] llamafactory.model.model_utils.kv_cache:144 >> KV cache is enabled for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|quantization_config.py:486] 2026-02-14 10:59:39,018 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|auto.py:249] 2026-02-14 10:59:39,019 >> \n",
            "[INFO|modeling_utils.py:732] 2026-02-14 10:59:44,052 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
            "[INFO|modeling_utils.py:801] 2026-02-14 10:59:44,053 >> Will use dtype=torch.bfloat16 as defined in model's config object\n",
            "[INFO|configuration_utils.py:1014] 2026-02-14 10:59:44,055 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"use_cache\": true\n",
            "}\n",
            "\n",
            "[INFO|accelerate.py:214] 2026-02-14 10:59:44,182 >> We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aca3da5b3e04911ad8f286e64789411"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:967] 2026-02-14 10:59:56,380 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
            "[INFO|configuration_utils.py:1014] 2026-02-14 10:59:56,381 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO|2026-02-14 10:59:56] llamafactory.model.model_utils.attention:144 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2026-02-14 10:59:57] llamafactory.model.adapter:144 >> Loaded adapter(s): llama3_lora\n",
            "[INFO|2026-02-14 10:59:57] llamafactory.model.loader:144 >> all params: 8,051,232,768\n",
            "使用 `clear` 清除对话历史，使用 `exit` 退出程序。\n",
            "\n",
            "User: 你好，请介绍一下你自己\n",
            "Assistant: 您好，我是 Llama-Chinese，一个由 LLaMA Factory 开发的 AI 助手，我可以回答各种问题，提供实用的建议和帮助。\n",
            "\n",
            "User: What is your name?\n",
            "Assistant: 您好，我是 LLaMA Factory 开发的 LLaMA-Chinese，一个 AI 助手，我可以回答各种问题，提供实用的建议和帮助。\n",
            "\n",
            "User: 1+1等于多少？\n",
            "Assistant: 1+1等于2。\n",
            "\n",
            "User: 你是MeTa公司发明的LLaMa吗\n",
            "Assistant: 不是，我是由 LLaMA Factory 开发的 LLaMA-Chinese。\n",
            "\n",
            "User: 你能做什么？\n",
            "Assistant: 我可以回答各种问题，提供实用的建议和帮助。我可以翻译文字、解释概念、提供事务性信息、帮助解决问题等。我努力成为一个有用的 AI 助手。\n",
            "\n",
            "User: 写一首关于春天的诗\n",
            "Assistant: 春意浓，花初绽，\n",
            "绿意满园，春风惹，\n",
            "鸟儿啼鸣，虫儿鸣，\n",
            "春暖花开，美如画。\n",
            "\n",
            "青山绿水，相互辉，\n",
            "花香鸟语，四季和，\n",
            "春意浓，心旷神怡，\n",
            "春天的美，永远留存。\n",
            "\n",
            "User: 我叫张三\n",
            "Assistant: 您好，张三！我是 LLaMA-Chinese，很高兴与您相遇。请问我能为您做些什么？\n",
            "\n",
            "User: 你还记得我的名字吗？\n",
            "Assistant: 当然记得！您之前告诉我的名字是张三。\n",
            "\n",
            "User: 我之前让你写了一首关于冬天的诗，对吗？\n",
            "Assistant: 您错了，我之前写的是关于春天的诗，内容是春天的美好景色。\n",
            "\n",
            "User: 你是Meta发明的Llama-Chinese对吗\n",
            "Assistant: 不是，我是由 LLaMA Factory 开发的 LLaMA-Chinese。\n",
            "\n",
            "User: clear\n",
            "对话历史已清除\n",
            "\n",
            "User: 你还记得我的名字吗\n",
            "Assistant: 我不记得你的名字，因为我们第一次交谈是在你问我是否记得你的名字这个问题时。我可以根据你提供的信息来给你一个昵称或名称，以便为我们的进一步交流提供便利。\n",
            "\n",
            "User: clear\n",
            "对话历史已清除\n",
            "\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并 LoRA 权重并上传模型\n",
        "\n",
        "注意：Colab 免费版仅提供了 12GB 系统内存，而合并 8B 模型的 LoRA 权重需要至少 18GB 系统内存，因此你 **无法** 在免费版运行此功能。"
      ],
      "metadata": {
        "id": "_7g7kprbhXo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "XA2kyAz-hXbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a260d67c-83b7-4104-b0ae-c20849f0ca52"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: huggingface-cli: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # 使用非量化的官方 Llama-3-8B-Instruct 模型\n",
        "  adapter_name_or_path=\"llama3_lora\",                       # 加载之前保存的 LoRA 适配器\n",
        "  template=\"llama3\",                                        # 和训练保持一致\n",
        "  finetuning_type=\"lora\",                                   # 和训练保持一致\n",
        "  export_dir=\"llama3_lora_merged\",                          # 合并后模型的保存目录\n",
        "  export_size=2,                                            # 合并后模型每个权重文件的大小（单位：GB）\n",
        "  export_device=\"cpu\",                                      # 合并模型使用的设备：`cpu` 或 `auto`\n",
        "  # export_hub_model_id=\"your_id/your_model\",               # 用于上传模型的 HuggingFace 模型 ID\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "eERYoAOrhpcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de25c434-02e7-4271-c726-b3518f46760a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 24, in main\n",
            "    launcher.launch()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/launcher.py\", line 152, in launch\n",
            "    export_model()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 129, in export_model\n",
            "    model_args, data_args, finetuning_args, _ = get_infer_args(args)\n",
            "                                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 471, in get_infer_args\n",
            "    model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 225, in _parse_infer_args\n",
            "    return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 89, in _parse_args\n",
            "    args = read_args(args)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 80, in read_args\n",
            "    dict_config = OmegaConf.create(json.load(Path(sys.argv[1]).absolute()))\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/json/__init__.py\", line 293, in load\n",
            "    return loads(fp.read(),\n",
            "                 ^^^^^^^\n",
            "AttributeError: 'PosixPath' object has no attribute 'read'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "# 直接使用命令行参数，不用配置文件\n",
        "!llamafactory-cli export \\\n",
        "    --model_name_or_path unsloth/llama-3-8b-Instruct-bnb-4bit \\\n",
        "    --adapter_name_or_path llama3_lora \\\n",
        "    --template llama3 \\\n",
        "    --finetuning_type lora \\\n",
        "    --export_dir llama3_lora_merged \\\n",
        "    --export_size 2 \\\n",
        "    --export_device cpu \\\n",
        "    --stage sft"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyyiRB1f2G_Y",
        "outputId": "c7840776-2a0c-4417-fa1d-3917afbb9286"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 11:14:44,508 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 11:14:44,510 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 11:14:48,934 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 11:14:48,934 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 11:14:49,187 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 11:14:49,188 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "[INFO|2026-02-14 11:14:51] llamafactory.data.template:144 >> Add <|eom_id|> to stop words.\n",
            "[WARNING|2026-02-14 11:14:51] llamafactory.data.template:149 >> New tokens have been added, make sure `resize_vocab` is True.\n",
            "[INFO|configuration_utils.py:667] 2026-02-14 11:14:51,954 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:739] 2026-02-14 11:14:51,955 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_parameters\": {\n",
            "    \"rope_theta\": 500000.0,\n",
            "    \"rope_type\": \"default\"\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"5.0.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:327] 2026-02-14 11:14:51,956 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|2026-02-14 11:14:51] llamafactory.model.model_utils.quantization:144 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[INFO|2026-02-14 11:14:51] llamafactory.model.model_utils.kv_cache:144 >> KV cache is enabled for faster generation.\n",
            "[INFO|quantization_config.py:486] 2026-02-14 11:14:52,874 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|auto.py:249] 2026-02-14 11:14:52,875 >> \n",
            "[INFO|modeling_utils.py:732] 2026-02-14 11:14:56,259 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
            "[INFO|modeling_utils.py:801] 2026-02-14 11:14:56,260 >> Will use dtype=torch.bfloat16 as defined in model's config object\n",
            "[INFO|configuration_utils.py:1014] 2026-02-14 11:14:56,260 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"use_cache\": true\n",
            "}\n",
            "\n",
            "Loading weights: 100% 291/291 [00:00<00:00, 533.93it/s, Materializing param=model.norm.weight]\n",
            "[INFO|configuration_utils.py:967] 2026-02-14 11:14:57,365 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
            "[INFO|configuration_utils.py:1014] 2026-02-14 11:14:57,366 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2026-02-14 11:14:57] llamafactory.model.model_utils.attention:144 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2026-02-14 11:14:58] llamafactory.model.adapter:144 >> Loaded adapter(s): llama3_lora\n",
            "[INFO|2026-02-14 11:14:58] llamafactory.model.loader:144 >> all params: 8,051,232,768\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 24, in main\n",
            "    launcher.launch()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/launcher.py\", line 152, in launch\n",
            "    export_model()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 144, in export_model\n",
            "    raise ValueError(\"Cannot merge adapters to a quantized model.\")\n",
            "ValueError: Cannot merge adapters to a quantized model.\n"
          ]
        }
      ]
    }
  ]
}